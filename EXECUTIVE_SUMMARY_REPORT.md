# AI Tutoring Model Evaluation: Executive Summary

**Date:** July 19, 2025  
**Models Evaluated:** Claude Haiku, GPT-4o Mini, Phi-3 Mini  
**Approaches Tested:** Zero-shot, Few-shot, Chain-of-Thought (CoT)  
**Dataset:** 20 mathematics tutoring scenarios across 5 subjects

---

## üéØ Key Finding: Few-shot GPT-4o Mini Shows Superior Performance

Few-shot GPT-4o Mini demonstrated the highest performance across evaluation metrics, with particularly notable effectiveness when responding to incorrect student answers.

### Performance Metrics
- **Overall Rating:** 3.60/5.0 (highest of all combinations)
- **High-Quality Response Rate:** 70% overall
- **Cost per Query:** $0.0002
- **Sample Size:** 20 test cases per model/approach combination

---

## üìä Methodology: Defining "High-Quality" Tutoring

**High-Quality Response:** Rating ‚â• 3.0 out of 5.0
- Indicates pedagogically sound tutoring guidance
- Measures effective teaching approach and empathy
- Independent of whether student's original answer was correct
- Focuses on AI tutoring effectiveness, not student performance

**Student Answer Baseline:** 50% of students gave correct answers initially (10/20 cases)

---

## üîç Critical Discovery: Performance Varies by Student Answer Correctness

### Key Insight
Few-shot GPT-4o Mini demonstrated significantly better performance when students provided incorrect answers compared to when students provided correct answers.

| Scenario | Few-shot GPT-4o Mini Performance |
|----------|-----------------------------------|
| **Student Correct** | 50% high-quality responses (Rating: 3.15) |
| **Student Incorrect** | **90% high-quality responses (Rating: 4.05)** |
| **Performance Difference** | +40% improvement with incorrect answers |

### Implications for Tutoring Systems
- Traditional expectations suggest tutors should perform better with correct student responses
- Few-shot prompting appears optimized for corrective instruction scenarios
- This pattern represents a significant finding for AI tutoring system design

---

## üìà Comparative Performance Analysis

**Visual References:**
- `answer_correctness_heatmap.png` - Comparative performance visualization
- `student_correctness_impact_analysis.png` - Impact analysis across all models

### High-Quality Response Rates by Approach

| Approach | Average Quality Rate | Best Performing Model | Range |
|----------|---------------------|----------------------|--------|
| **Few-shot** | **59.1%** | GPT-4o Mini (70%) | 47%-70% |
| Chain-of-Thought | 35.0% | GPT-4o Mini (40%) | 25%-40% |
| Zero-shot | 30.0% | GPT-4o Mini (40%) | 20%-40% |

### Performance by Student Answer Correctness

#### When Students Provide Correct Answers
| Model + Approach | Quality Rate | Mean Rating |
|------------------|--------------|-------------|
| Few-shot GPT-4o Mini | 50% | 3.15 |
| Zero-shot GPT-4o Mini | 70% | 3.20 |
| Zero-shot Claude Haiku | 60% | 2.95 |

#### When Students Provide Incorrect Answers
| Model + Approach | Quality Rate | Mean Rating |
|------------------|--------------|-------------|
| **Few-shot GPT-4o Mini** | **90%** | **4.05** |
| Few-shot Claude Haiku | 70% | 3.00 |
| Zero-shot GPT-4o Mini | 10% | 1.60 |
| Zero-shot Claude Haiku | 0% | 1.30 |

**Notable Pattern:** Zero-shot approaches show significant performance degradation when students provide incorrect answers, while few-shot approaches maintain or improve performance.

---

## üìö Subject-Specific Analysis

### Student Success Rate by Mathematical Domain
- **Geometry:** 60% correct responses (5 cases)
- **Algebra:** 50% correct responses (4 cases)
- **Elementary:** 50% correct responses (6 cases)
- **Trigonometry:** 50% correct responses (4 cases)
- **Calculus:** 0% correct responses (1 case)

### Tutoring System Implications
The distribution of student correctness across subjects demonstrates the importance of AI systems that can effectively handle incorrect responses, as this represents 50% or more of real tutoring scenarios.

---

## üî¨ Qualitative Observations

### Chain-of-Thought Limitations
Analysis revealed that CoT prompting tends to provide complete solutions rather than guided instruction, potentially reducing pedagogical effectiveness in tutoring contexts.

### Zero-shot Challenges
Zero-shot approaches occasionally generated non-existent conversation elements, creating potential confusion in tutoring interactions.

### Few-shot Advantages
Few-shot prompting demonstrated optimal balance of brevity, empathy, and pedagogical appropriateness, particularly in corrective tutoring scenarios.

---

## üìä Cost-Effectiveness Analysis

| Approach + Model | Cost per Query | Quality Rate | Cost-Effectiveness Ratio |
|------------------|----------------|--------------|-------------------------|
| Few-shot Phi-3 Mini | $0.0001 | 47% | 26,244 |
| Few-shot GPT-4o Mini | $0.0002 | 70% | 22,054 |
| Zero-shot Phi-3 Mini | $0.0001 | 20% | 23,688 |

Note: Cost-effectiveness calculated as quality rate divided by cost per query.

---

## üìã Research Methodology

- **Evaluation Scale:** 1-5 point rating system for tutoring quality
- **Quality Threshold:** ‚â•3.0 rating classified as high-quality response
- **Sample Size:** 20 test cases per model/approach combination
- **Rating Criteria:** Pedagogical approach, empathy, mathematical accuracy, appropriateness of guidance
- **Student Response Distribution:** 50% correct, 50% incorrect (balanced evaluation)

---

## üìä Supporting Data and Visualizations

### Generated Analysis Files:
- `answer_correctness_heatmap.png` - Performance comparison across models and approaches
- `student_correctness_impact_analysis.png` - Four-panel analysis of student correctness impact
- `accuracy_vs_rating.png` - Correlation analysis between performance metrics
- `enhanced_correctness_analysis.csv` - Complete quantitative results
- `student_correctness_impact_results.csv` - Detailed correctness impact data

---

## ÔøΩÔøΩ Key Research Findings

1. **Few-shot prompting demonstrates superior performance** across multiple evaluation metrics
2. **Student answer correctness significantly impacts AI tutor performance**, with most approaches showing degraded performance on incorrect responses
3. **Few-shot GPT-4o Mini uniquely excels with incorrect student answers**, representing an important finding for tutoring system design
4. **Cost-effectiveness varies significantly** across model and approach combinations
5. **Subject domain affects student success rates**, but AI tutor performance patterns remain consistent

---

## üìñ Conclusion

This evaluation demonstrates significant performance differences across AI tutoring approaches, with few-shot prompting showing particular promise for educational applications. The finding that few-shot GPT-4o Mini performs better with incorrect student responses represents a valuable insight for the design of AI tutoring systems, as real educational scenarios frequently involve students requiring correction and guidance.

Future research directions may include investigating the mechanisms underlying few-shot prompting's effectiveness in corrective tutoring scenarios and exploring optimization strategies for other approaches in educational contexts.
